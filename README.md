# EasyPPO
This repository features a straightforward and user-friendly implementation of Proximal Policy Optimization (PPO) integrated with a custom Mujoco environment and Weights & Biases (WandB) logging. Designed with simplicity in mind, it serves as an excellent starting point for researchers who are new to PPO or seeking an easy-to-understand implementation.

## Overview
The PPO implementation provided in this repository was developed in The Movement Lab at Stanford. Its primary features include:

* A simple and clean PPO implementation, perfect for beginners or those looking for a minimalistic codebase.
* A custom Mujoco environment example (training a humanoid to walk using DeepMimic) to help you understand how to adapt PPO for your own use cases.
* Integrated WandB logging for seamless performance tracking and visualization during training.
## Getting Started
To get started with this implementation, follow these steps:
1. Clone the repository
2. Install the required dependencies
3. Run the example script
4. Customize the codebase for your specific needs
